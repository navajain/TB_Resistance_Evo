{"cells":[{"cell_type":"markdown","metadata":{"id":"QB4dt9R_udWM"},"source":["# Summary\n","\n","Idea: We have seen decent signals from the 0th and last index of our embeddings; what if we used the average of our embeddings?"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3242,"status":"ok","timestamp":1716947567370,"user":{"displayName":"Benson Kung","userId":"09507617451673917234"},"user_tz":420},"id":"MBfnUM6bpyxN","outputId":"1ace38ab-f407-4dc5-9f40-c718b71380ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["\"\"\"\n","Mount Drive onto this notebook. Note that there is a specific\n","file structure. In particular, we have\n","-- MyDrive\n","-- -- EVO\n","-- -- -- vcfs\n","-- -- -- CRyPTIC_reuse_table_20231208.csv\n","-- -- -- h37rv_genebank_flatfile.gbff\n","\n","(This will need to be updated!)\n","\"\"\"\n","\n","import os\n","\n","import pandas as pd\n","import numpy as np\n","import sklearn as sk\n","\n","from tqdm import tqdm\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","evo_general_dir = '/content/drive/MyDrive/EVO/'\n","samples_dir = 'vcfs/'\n","\n","emb_dir = evo_general_dir + 'rif_embeddings_v1/'\n","current_dir = emb_dir + 'embeds_1.0_singles_partial/'\n","\n","h37rv_genome_file = 'GCF_000195955.2_ASM19595v2_genomic.fna'\n","cryptic_general_file = 'CRyPTIC_reuse_table_20231208.csv'\n","\n","unique_ids = np.load(emb_dir + 'unique_ids.npy')\n","reuse_df = pd.read_csv(evo_general_dir + cryptic_general_file)\n","\n","df = pd.read_csv(emb_dir + 'rif_labels.csv')[['UNIQUEID', 'RIF_BINARY_PHENOTYPE']]\n","embs_df = df.dropna()"]},{"cell_type":"markdown","metadata":{"id":"N6KG9IJ6u6Ey"},"source":["## Utilities"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1716947567370,"user":{"displayName":"Benson Kung","userId":"09507617451673917234"},"user_tz":420},"id":"W4mSAH0au7be"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","from os.path import exists\n","\n","\n","def get_site(unique_id):\n","  return unique_id.split('.')[1]\n","\n","\n","def embedding_generator(dir, unique_ids, ind=0, avg=False):\n","  \"\"\"\n","    Generator for obtaining embeddings from a directory.\n","  \"\"\"\n","  for id in unique_ids:\n","    site = 'site_' + get_site(id) + '/'\n","    sample_dir = dir + site + id + '.npy'\n","\n","    if avg:\n","      yield np.mean(np.load(sample_dir), axis=0)\n","    else:\n","      yield np.load(sample_dir)[ind]\n","\n","\n","def compute_confusion(model, data_dir, test_df, save_file_path, pheno_col):\n","  \"\"\"\n","    Compute confusion matrix for a given model.\n","  \"\"\"\n","  preds = []\n","  for i, embed in enumerate(embedding_generator(\n","                                      data_dir,\n","                                      test_df['UNIQUEID'],\n","                                      avg=True\n","                                      )):\n","    preds.append(model.predict(embed.reshape(1, -1)))\n","\n","  matrix = confusion_matrix(test_df[pheno_col], preds)\n","  np.save(save_file_path, matrix)\n","\n","  return matrix"]},{"cell_type":"markdown","metadata":{"id":"mH2hY-S0wQaY"},"source":["# Good ol' Logistic Regression"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":18204,"status":"ok","timestamp":1716947585569,"user":{"displayName":"Benson Kung","userId":"09507617451673917234"},"user_tz":420},"id":"fsaiMZObDJNl"},"outputs":[],"source":["test = []\n","for i, row in embs_df.iterrows():\n","  if exists(current_dir + 'site_' + get_site(row['UNIQUEID']) + '/' + row['UNIQUEID'] + '.npy'):\n","    test.append(row)\n","test = pd.DataFrame(test)\n","embs_df = test"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":199,"status":"ok","timestamp":1716947585750,"user":{"displayName":"Benson Kung","userId":"09507617451673917234"},"user_tz":420},"id":"YbL6hoU9v2Xz"},"outputs":[],"source":["from sklearn.linear_model import SGDClassifier, LogisticRegression\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import train_test_split\n","\n","train_df, test_df = train_test_split(embs_df, test_size=0.2)\n","test_df, val_df = train_test_split(test_df, test_size=0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"vGkdNdu4u0Yd"},"outputs":[{"name":"stderr","output_type":"stream","text":["676it [08:35,  1.31it/s]\n"]},{"ename":"ValueError","evalue":"cannot reshape array of size 12272 into shape (500,4096)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-5-8ffffd9eee8e\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 14\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 15\u001b[0;31m   for i, embed in tqdm(enumerate(\n\u001b[0m\u001b[1;32m     16\u001b[0m                         embedding_generator(\n\u001b[1;32m     17\u001b[0m                             \u001b[0mcurrent_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-2-22ffcd9f5e6a\u003e\u001b[0m in \u001b[0;36membedding_generator\u001b[0;34m(dir, unique_ids, ind, avg)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 18\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    454\u001b[0m                                           max_header_size=max_header_size)\n\u001b[1;32m    455\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 456\u001b[0;31m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[1;32m    457\u001b[0m                                          \u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                                          max_header_size=max_header_size)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 839\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 12272 into shape (500,4096)"]}],"source":["import gc\n","\n","pheno_col = 'RIF_BINARY_PHENOTYPE'\n","prepend = 'avg_'\n","\n","weights = compute_class_weight(class_weight='balanced',\n","                               classes=np.unique(embs_df[pheno_col]),\n","                               y=embs_df[pheno_col])\n","weights = dict(zip(np.unique(embs_df[pheno_col]), weights))\n","lr = SGDClassifier(loss='log_loss', class_weight=weights)\n","\n","confusions = []\n","errors = []\n","for k in range(10):\n","  for i, embed in tqdm(enumerate(\n","                        embedding_generator(\n","                            current_dir,\n","                            train_df['UNIQUEID'],\n","                            avg=True\n","                            )\n","                        )):\n","    if len(embed) != 4096:\n","      errors.append((i, embed))\n","      continue\n","\n","    lr.partial_fit(embed.reshape(1, -1),\n","                        [train_df[pheno_col].iloc[i]],\n","                        classes=np.unique(embs_df[pheno_col]))\n","    del embed\n","    gc.collect()\n","\n","  matrix = compute_confusion(lr,\n","                             current_dir,\n","                             val_df,\n","                             current_dir + prepend + 'lr_confusion_' + str(k) + '.npy',\n","                             pheno_col)\n","  confusions.append(matrix)\n","  print(matrix)\n","\n","np.save(emb_dir + prepend + 'lr_model.npy', lr)\n","np.save(emb_dir + prepend+ 'lr_confusions.npy', confusions)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO3AQ/6ZBq0p76OHb4TCWE9","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}